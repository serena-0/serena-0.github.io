[
  {
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "url": "https://huggingface.co/papers/2512.20605",
    "source": "HF Trending Papers",
    "published_at": "2025-12-26",
    "raw_snippet": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 15,
    "sources": [
      "HF Trending Papers"
    ]
  },
  {
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "url": "https://huggingface.co/papers/2512.15716",
    "source": "HF Trending Papers",
    "published_at": "2025-12-26",
    "raw_snippet": "Spatia, a spatial memory-aware video generation framework, maintains long-term spatial and temporal consistency by preserving and updating a 3D scene point cloud, enabling realistic video generation and interactive editing. AI-generated summary Existing video generation models struggle to maintain long-term spatial and…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 15,
    "sources": [
      "HF Trending Papers"
    ]
  },
  {
    "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
    "url": "https://huggingface.co/papers/2512.19995",
    "source": "HF Trending Papers",
    "published_at": "2025-12-26",
    "raw_snippet": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models),…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 15,
    "sources": [
      "HF Trending Papers"
    ]
  },
  {
    "title": "How Much 3D Do Video Foundation Models Encode?",
    "url": "https://huggingface.co/papers/2512.19949",
    "source": "HF Trending Papers",
    "published_at": "2025-12-26",
    "raw_snippet": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 15,
    "sources": [
      "HF Trending Papers"
    ]
  },
  {
    "title": "VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
    "url": "https://huggingface.co/papers/2512.19680",
    "source": "HF Trending Papers",
    "published_at": "2025-12-26",
    "raw_snippet": "VA-$\\pi$ optimizes autoregressive visual generators using a pixel-space objective to improve image quality and performance without retraining tokenizers or using external rewards. AI-generated summary Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, token…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 15,
    "sources": [
      "HF Trending Papers"
    ]
  },
  {
    "title": "Latent Implicit Visual Reasoning",
    "url": "https://huggingface.co/papers/2512.21218",
    "source": "HF Trending Papers",
    "published_at": "2025-12-26",
    "raw_snippet": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervisin…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 15,
    "sources": [
      "HF Trending Papers"
    ]
  },
  {
    "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
    "url": "https://huggingface.co/papers/2512.13043",
    "source": "HF Trending Papers",
    "published_at": "2025-12-26",
    "raw_snippet": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Disti…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 15,
    "sources": [
      "HF Trending Papers"
    ]
  },
  {
    "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
    "url": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
    "source": "Hugging Face Blog",
    "published_at": "2025-12-23",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 12,
    "sources": [
      "Hugging Face Blog"
    ]
  },
  {
    "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
    "url": "https://huggingface.co/blog/tokenizers",
    "source": "Hugging Face Blog",
    "published_at": "2025-12-18",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 11,
    "sources": [
      "Hugging Face Blog"
    ]
  },
  {
    "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator",
    "url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe",
    "source": "Hugging Face Blog",
    "published_at": "2025-12-17",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 11,
    "sources": [
      "Hugging Face Blog"
    ]
  },
  {
    "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents",
    "url": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face",
    "source": "Hugging Face Blog",
    "published_at": "2025-12-15",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 11,
    "sources": [
      "Hugging Face Blog"
    ]
  },
  {
    "title": "VLM能否看清「分子世界」？人大联合清华、达摩院等机构发布首个微观空间智能基准MiSI-Bench",
    "url": "https://www.jiqizhixin.com/articles/2025-12-26-7",
    "source": "机器之心",
    "published_at": "2025-12-26",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 6,
    "sources": [
      "机器之心"
    ]
  },
  {
    "title": "视频生成DeepSeek时刻！清华&生数开源框架提速200倍，一周斩获2k Star",
    "url": "https://www.jiqizhixin.com/articles/2025-12-26-6",
    "source": "机器之心",
    "published_at": "2025-12-26",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 6,
    "sources": [
      "机器之心"
    ]
  },
  {
    "title": "Agent「记吃不记打」？华为诺亚&港中文发布SCOPE：Prompt自我进化，让HLE成功率翻倍",
    "url": "https://www.jiqizhixin.com/articles/2025-12-26-5",
    "source": "机器之心",
    "published_at": "2025-12-26",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 6,
    "sources": [
      "机器之心"
    ]
  },
  {
    "title": "BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization",
    "url": "https://arxiv.org/abs/2512.20623",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20623v1 Announce Type: new Abstract: Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment",
    "url": "https://arxiv.org/abs/2512.20624",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20624v1 Announce Type: new Abstract: This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinat…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation",
    "url": "https://arxiv.org/abs/2512.20626",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20626v1 Announce Type: new Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holist…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)",
    "url": "https://arxiv.org/abs/2512.20628",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20628v1 Announce Type: new Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, prov…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data",
    "url": "https://arxiv.org/abs/2512.20630",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20630v1 Announce Type: new Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment usin…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Erkang-Diagnosis-1.1 Technical Report",
    "url": "https://arxiv.org/abs/2512.20632",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20632v1 Announce Type: new Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approa…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning",
    "url": "https://arxiv.org/abs/2512.20647",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20647v1 Announce Type: new Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "AIAuditTrack: A Framework for AI Security system",
    "url": "https://arxiv.org/abs/2512.20649",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20649v1 Announce Type: new Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA",
    "url": "https://arxiv.org/abs/2512.20650",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20650v1 Announce Type: new Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Mu…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence",
    "url": "https://arxiv.org/abs/2512.20651",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20651v1 Announce Type: new Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized se…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  }
]