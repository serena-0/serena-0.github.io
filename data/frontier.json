[
  {
    "title": "AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems",
    "url": "https://huggingface.co/blog/ServiceNow-AI/aprielguard",
    "source": "Hugging Face Blog",
    "published_at": "2025-12-23",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 13,
    "sources": [
      "Hugging Face Blog"
    ]
  },
  {
    "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
    "url": "https://huggingface.co/blog/tokenizers",
    "source": "Hugging Face Blog",
    "published_at": "2025-12-18",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 11,
    "sources": [
      "Hugging Face Blog"
    ]
  },
  {
    "title": "The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator",
    "url": "https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe",
    "source": "Hugging Face Blog",
    "published_at": "2025-12-17",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 11,
    "sources": [
      "Hugging Face Blog"
    ]
  },
  {
    "title": "CUGA on Hugging Face: Democratizing Configurable AI Agents",
    "url": "https://huggingface.co/blog/ibm-research/cuga-on-hugging-face",
    "source": "Hugging Face Blog",
    "published_at": "2025-12-15",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 11,
    "sources": [
      "Hugging Face Blog"
    ]
  },
  {
    "title": "VLM能否看清「分子世界」？人大联合清华、达摩院等机构发布首个微观空间智能基准MiSI-Bench",
    "url": "https://www.jiqizhixin.com/articles/2025-12-26-7",
    "source": "机器之心",
    "published_at": "2025-12-26",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 6,
    "sources": [
      "机器之心"
    ]
  },
  {
    "title": "视频生成DeepSeek时刻！清华&生数开源框架提速200倍，一周斩获2k Star",
    "url": "https://www.jiqizhixin.com/articles/2025-12-26-6",
    "source": "机器之心",
    "published_at": "2025-12-26",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 6,
    "sources": [
      "机器之心"
    ]
  },
  {
    "title": "Agent「记吃不记打」？华为诺亚&港中文发布SCOPE：Prompt自我进化，让HLE成功率翻倍",
    "url": "https://www.jiqizhixin.com/articles/2025-12-26-5",
    "source": "机器之心",
    "published_at": "2025-12-26",
    "raw_snippet": "",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 6,
    "sources": [
      "机器之心"
    ]
  },
  {
    "title": "BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization",
    "url": "https://arxiv.org/abs/2512.20623",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20623v1 Announce Type: new Abstract: Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment",
    "url": "https://arxiv.org/abs/2512.20624",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20624v1 Announce Type: new Abstract: This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinat…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation",
    "url": "https://arxiv.org/abs/2512.20626",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20626v1 Announce Type: new Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holist…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)",
    "url": "https://arxiv.org/abs/2512.20628",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20628v1 Announce Type: new Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, prov…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data",
    "url": "https://arxiv.org/abs/2512.20630",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20630v1 Announce Type: new Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment usin…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Erkang-Diagnosis-1.1 Technical Report",
    "url": "https://arxiv.org/abs/2512.20632",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20632v1 Announce Type: new Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approa…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning",
    "url": "https://arxiv.org/abs/2512.20647",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20647v1 Announce Type: new Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "AIAuditTrack: A Framework for AI Security system",
    "url": "https://arxiv.org/abs/2512.20649",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20649v1 Announce Type: new Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA",
    "url": "https://arxiv.org/abs/2512.20650",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20650v1 Announce Type: new Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Mu…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  },
  {
    "title": "Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence",
    "url": "https://arxiv.org/abs/2512.20651",
    "source": "arXiv cs.AI",
    "published_at": "2025-12-26",
    "raw_snippet": "arXiv:2512.20651v1 Announce Type: new Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized se…",
    "ai_summary": "",
    "ai_generated": false,
    "tags": [],
    "hot_score": 5,
    "sources": [
      "arXiv cs.AI"
    ]
  }
]